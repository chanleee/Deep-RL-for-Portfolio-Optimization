import os
from time import sleep
from collections import deque, namedtuple

import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from tensorboardX import SummaryWriter
import matplotlib.pyplot as plt
from IPython import display
from tqdm import tqdm # í›ˆë ¨ ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•´ tqdm ì„í¬íŠ¸

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸: ìƒˆë¡œìš´ modelsì™€ memoryë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
from memory import ReplayBuffer # ìˆ˜ì •ëœ memory.pyì˜ ReplayBuffer í´ë˜ìŠ¤
from models import Actor, Critic # ìˆ˜ì •ëœ models.pyì˜ Actor, Critic í´ë˜ìŠ¤

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
BUFFER_SIZE = int(1e6)  # ë¦¬í”Œë ˆì´ ë²„í¼ í¬ê¸°
BATCH_SIZE = 128        # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
GAMMA = 0.99            # í• ì¸ ê³„ìˆ˜ (Discount factor)
TAU = 1e-2              # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸ ê³„ìˆ˜
LR_ACTOR = 1e-5         # Actor í•™ìŠµë¥ 
LR_CRITIC = 1e-4        # Critic í•™ìŠµë¥ 
WEIGHT_DECAY = 0        # L2 ê°€ì¤‘ì¹˜ ê°ì‡  (Critic)
EXPLORATION_STD = 0.1   # í–‰ë™ íƒìƒ‰ì„ ìœ„í•œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì˜ í‘œì¤€í¸ì°¨
LEARN_EVERY = 1         # ëª‡ ìŠ¤í…ë§ˆë‹¤ í•™ìŠµí• ì§€ ê²°ì •
LEARN_NUM = 1           # í•œ ë²ˆ í•™ìŠµí•  ë•Œ ëª‡ ë²ˆì˜ ë°°ì¹˜ë¥¼ í•™ìŠµí• ì§€ ê²°ì •

class Agent():
    """DDPG ì—ì´ì „íŠ¸: í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤."""

    def __init__(self, state_size, action_size, random_seed):
        """
        Agent ê°ì²´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            state_size (int): ìƒíƒœ ê³µê°„ì˜ ì°¨ì›
            action_size (int): í–‰ë™ ê³µê°„ì˜ ì°¨ì›
            random_seed (int): ë‚œìˆ˜ ì‹œë“œ
        """
        self.state_size = state_size
        self.action_size = action_size
        self.seed = np.random.seed(random_seed)
        torch.manual_seed(random_seed)

        # Actor Network (ë¡œì»¬ ë° íƒ€ê²Ÿ)
        self.actor_local = Actor(state_size, action_size).to(device)
        self.actor_target = Actor(state_size, action_size).to(device)
        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)

        # Critic Network (ë¡œì»¬ ë° íƒ€ê²Ÿ)
        self.critic_local = Critic(state_size, action_size).to(device)
        self.critic_target = Critic(state_size, action_size).to(device)
        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)

        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ì™€ ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ì´ˆê¸°í™”
        self.hard_update(self.actor_local, self.actor_target)
        self.hard_update(self.critic_local, self.critic_target)

        # ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, device, random_seed)
        
        # íƒ€ì„ìŠ¤í… ì¹´ìš´í„°
        self.t_step = 0

    def train(self, train_env, validation_env, n_episodes, model_path):
        """
        DDPG ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨ì‹œí‚¤ê³ , ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            train_env (gym.Env): í›ˆë ¨ì— ì‚¬ìš©í•  í™˜ê²½.
            validation_env (gym.Env): ê²€ì¦ì— ì‚¬ìš©í•  í™˜ê²½.
            n_episodes (int): í›ˆë ¨í•  ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜.
            model_path (str): ìµœì  ëª¨ë¸ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        """
        scores_deque = deque(maxlen=10) # ìµœê·¼ 10ê°œ ìŠ¤ì½”ì–´ë§Œ ì €ì¥
        scores = []
        
        best_validation_score = -np.inf # ìµœê³  ê²€ì¦ ì ìˆ˜ ì´ˆê¸°í™”
        
        # tqdmì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë£¨í”„ ì§„í–‰ë¥  í‘œì‹œ
        progress_bar = tqdm(range(1, n_episodes + 1), desc="Training Progress")
        for i_episode in progress_bar:
            state = train_env.reset()
            score = 0
            done = False
            
            while not done:
                action = self.act(state)
                next_state, reward, done, _ = train_env.step(action)
                self.step(state, action, reward, next_state, done)
                
                state = next_state
                score += reward
                if done:
                    break
            
            scores_deque.append(score)
            scores.append(score)
            
            avg_score = np.mean(scores_deque)
            progress_bar.set_postfix({"Avg Train Score": f"{avg_score:.2f}"})

            # 5 ì—í”¼ì†Œë“œë§ˆë‹¤ ê²€ì¦ ë° ëª¨ë¸ ì €ì¥
            if i_episode % 5 == 0:
                # evaluate_for_validation í•¨ìˆ˜ëŠ” ìƒ¤í”„ ì§€ìˆ˜ë¥¼ ë°˜í™˜
                validation_score = evaluate_for_validation(validation_env, self)
                tqdm.write(f"\nEpisode {i_episode}\t"
                           f"Avg Train Score: {avg_score:.2f}\t"
                           f"Validation Sharpe Ratio: {validation_score:.4f}")

                # ìµœê³  ê²€ì¦ ì ìˆ˜ë¥¼ ê°±ì‹ í•˜ë©´ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
                if validation_score > best_validation_score:
                    best_validation_score = validation_score
                    tqdm.write(f"ğŸ‰ New best model found! Saving model to {model_path}")
                    torch.save(self.actor_local.state_dict(), os.path.join(model_path, 'best_actor.pth'))
                    torch.save(self.critic_local.state_dict(), os.path.join(model_path, 'best_critic.pth'))

    def step(self, state, action, reward, next_state, done):
        """ê²½í—˜ì„ ë¦¬í”Œë ˆì´ ë²„í¼ì— ì €ì¥í•˜ê³ , ì£¼ê¸°ì ìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ê²½í—˜ ì €ì¥
        self.memory.add(state, action, reward, next_state, done)

        # ì •í•´ì§„ ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ìˆ˜í–‰
        self.t_step = (self.t_step + 1) % LEARN_EVERY
        if self.t_step == 0:
            if len(self.memory) > BATCH_SIZE:
                for _ in range(LEARN_NUM):
                    experiences = self.memory.sample()
                    self.learn(experiences, GAMMA)

    def act(self, state, use_exploration=True):
        """ì£¼ì–´ì§„ ìƒíƒœì— ëŒ€í•´ í–‰ë™(í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜)ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        self.actor_local.eval()
        with torch.no_grad():
            # actionì€ ì´ì œ (action_size,) í˜•íƒœì˜ ë²¡í„°
            action = self.actor_local(state).cpu().data.numpy().flatten()
        self.actor_local.train()

        if use_exploration:
            # OU Noise ëŒ€ì‹  ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = np.random.normal(0, EXPLORATION_STD, size=self.action_size)
            action += noise
        
        # í–‰ë™ í›„ì²˜ë¦¬: ëª¨ë¸ ì¶œë ¥ì¸µì— Softmaxê°€ ìˆë”ë¼ë„ ë…¸ì´ì¦ˆë¡œ ì¸í•´ ì œì•½ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        # clipê³¼ ì •ê·œí™”ëŠ” ì•ˆì „ì¥ì¹˜ë¡œ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        action = np.clip(action, 0, 1)
        action /= (action.sum() + 1e-8) # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€

        return action

    def learn(self, experiences, gamma):
        """
        ì£¼ì–´ì§„ ê²½í—˜ ë°°ì¹˜(batch)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¹˜ í•¨ìˆ˜ì™€ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        (ì´ ë¶€ë¶„ì˜ ë¡œì§ì€ í‘œì¤€ DDPG ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë²¡í„° ë°ì´í„°ì—ì„œë„ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.)
        """
        states, actions, rewards, next_states, dones = experiences

        # ---------------- CRITIC ì—…ë°ì´íŠ¸ ---------------- #
        # íƒ€ê²Ÿ Actorë¡œë¶€í„° ë‹¤ìŒ í–‰ë™ì„ ì˜ˆì¸¡
        actions_next = self.actor_target(next_states)
        # íƒ€ê²Ÿ Criticìœ¼ë¡œë¶€í„° ë‹¤ìŒ ìƒíƒœ-í–‰ë™ ìŒì˜ Q-valueë¥¼ ê³„ì‚°
        Q_targets_next = self.critic_target(next_states, actions_next)
        # í˜„ì¬ ìƒíƒœì— ëŒ€í•œ Q-target ê³„ì‚° (y_i)
        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))
        # ë¡œì»¬ Criticìœ¼ë¡œë¶€í„° í˜„ì¬ Q-value ì˜ˆì¸¡
        Q_expected = self.critic_local(states, actions)
        # Critic ì†ì‹¤ ê³„ì‚° (MSE)
        critic_loss = F.mse_loss(Q_expected, Q_targets)
        # Critic ì—…ë°ì´íŠ¸
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1) # Gradient Clipping
        self.critic_optimizer.step()

        # ---------------- ACTOR ì—…ë°ì´íŠ¸ ----------------- #
        # ë¡œì»¬ Actorë¡œë¶€í„° í–‰ë™ ì˜ˆì¸¡
        actions_pred = self.actor_local(states)
        # Actor ì†ì‹¤ ê³„ì‚° (Criticì´ ì˜ˆì¸¡í•œ Q-valueë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ)
        actor_loss = -self.critic_local(states, actions_pred).mean()
        # Actor ì—…ë°ì´íŠ¸
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # ---------------- TARGET ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ----------------- #
        self.soft_update(self.critic_local, self.critic_target, TAU)
        self.soft_update(self.actor_local, self.actor_target, TAU)

    def soft_update(self, local_model, target_model, tau):
        """
        íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ë¥¼ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target
        """
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)
            
    def hard_update(self, local_model, target_model):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ë¥¼ ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ì™€ ë™ì¼í•˜ê²Œ í•˜ë“œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(local_param.data)