# agent.py (PPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì „ë©´ ìˆ˜ì •)

import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal
import os
from tqdm import tqdm
from evaluation import evaluate_for_validation
from models import ActorCritic
import numpy as np

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class Memory:
    # PPOëŠ” On-Policy ì•Œê³ ë¦¬ì¦˜ì´ë¯€ë¡œ, ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ë©”ëª¨ë¦¬ë¥¼ ë¹„ì›ë‹ˆë‹¤.
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init):
        
        self.action_std = action_std_init
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs
        
        self.policy = ActorCritic(state_dim, action_dim, action_std_init).to(device)
        self.optimizer = torch.optim.Adam([
                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},
                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}
                    ])

        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init).to(device)
        self.policy_old.load_state_dict(self.policy.state_dict())
        
        self.MseLoss = nn.MSELoss()

    def set_action_std(self, new_action_std):
        self.action_std = new_action_std
        self.policy.set_action_std(new_action_std)
        self.policy_old.set_action_std(new_action_std)

    def act(self, state, memory=None):
        """
        ì£¼ì–´ì§„ ìƒíƒœì— ëŒ€í•´ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.
        í›ˆë ¨ ì‹œ(memory != None): í™•ë¥ ì ìœ¼ë¡œ í–‰ë™ì„ ìƒ˜í”Œë§í•˜ê³  ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
        í‰ê°€ ì‹œ(memory == None): ê°€ì¥ í™•ë¥  ë†’ì€ í–‰ë™(í‰ê· )ì„ ê²°ì •ë¡ ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        with torch.no_grad():
            state = torch.FloatTensor(state).to(device)
            action_mean = self.policy_old.actor(state)
            cov_mat = torch.diag(self.policy_old.action_var).unsqueeze(dim=0)
            dist = MultivariateNormal(action_mean, cov_mat)

            # í›ˆë ¨ ì‹œì—ëŠ” ìƒ˜í”Œë§, í‰ê°€ ì‹œì—ëŠ” í‰ê· ê°’(ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í–‰ë™) ì‚¬ìš©
            if memory is not None:
                action = dist.sample()
                action_logprob = dist.log_prob(action)
                memory.states.append(state)
                memory.actions.append(action)
                memory.logprobs.append(action_logprob)
            else:
                action = action_mean

        # ì •ê·œí™”í•˜ì—¬ ê°€ì¤‘ì¹˜ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ í•¨
        action = torch.clamp(action, 0, 1)
        action = action / (action.sum(dim=-1, keepdim=True) + 1e-8)

        return action.detach().cpu().numpy().flatten()

    def evaluate(self, state, action):
        return self.policy.evaluate(state, action)

    def update(self, memory):
        # ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ì‹ìœ¼ë¡œ ë³´ìƒ-íˆ¬-ê³ (Reward-to-go) ê³„ì‚°
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
        
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)

        old_states = torch.squeeze(torch.stack(memory.states, dim=0)).detach().to(device)
        old_actions = torch.squeeze(torch.stack(memory.actions, dim=0)).detach().to(device)
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=0)).detach().to(device)
        
        # K ì—í¬í¬ ë™ì•ˆ ì •ì±… ìµœì í™”
        for _ in range(self.K_epochs):
            logprobs, state_values, dist_entropy = self.evaluate(old_states, old_actions)
            
            advantages = rewards - state_values.detach()
            ratios = torch.exp(logprobs - old_logprobs.detach())
            
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages
            
            # --- ìˆ˜ì •ëœ ë¶€ë¶„: state_values.squeeze()ë¡œ ì°¨ì› ì¶•ì†Œ ---
            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values.squeeze(), rewards) - 0.01 * dist_entropy
            
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
            
        self.policy_old.load_state_dict(self.policy.state_dict())
        memory.clear_memory()

    def train(self, train_env, validation_env, max_training_timesteps, update_timestep, model_path):
        # PPOëŠ” ì—í”¼ì†Œë“œ ê¸°ë°˜ì´ ì•„ë‹Œ íƒ€ì„ìŠ¤í… ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
        memory = Memory()
        
        timestep = 0
        best_validation_score = -np.inf

        progress_bar = tqdm(range(1, int(max_training_timesteps)+1), desc="Training Timesteps (PPO)")
        
        while timestep < max_training_timesteps:
            state = train_env.reset()
            done = False
            
            while not done and timestep < max_training_timesteps:
                # í˜„ì¬ íƒ€ì„ìŠ¤í…ì—ì„œ í–‰ë™ ê²°ì •
                action = self.act(state, memory)
                state, reward, done, _ = train_env.step(action)
                
                # ë©”ëª¨ë¦¬ì— ë³´ìƒê³¼ ì¢…ë£Œ ì—¬ë¶€ ì €ì¥
                memory.rewards.append(reward)
                memory.is_terminals.append(done)
                
                timestep += 1
                progress_bar.update(1)
                
                # ì¼ì • íƒ€ì„ìŠ¤í…ë§ˆë‹¤ ì •ì±… ì—…ë°ì´íŠ¸
                if timestep % update_timestep == 0:
                    self.update(memory)

                # ì£¼ê¸°ì ìœ¼ë¡œ ê²€ì¦ ë° ëª¨ë¸ ì €ì¥
                if timestep % (update_timestep * 5) == 0: # 5ë²ˆ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ê²€ì¦
                    validation_score = evaluate_for_validation(validation_env, self)
                    tqdm.write(f"\nTimestep {timestep}\tValidation Sharpe Ratio: {validation_score:.4f}")
                    if validation_score > best_validation_score:
                        best_validation_score = validation_score
                        tqdm.write(f"ğŸ‰ New best model found! Saving model to {model_path}")
                        torch.save(self.policy.state_dict(), os.path.join(model_path, 'best_ppo_model.pth'))