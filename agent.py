import os
import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from tqdm import tqdm

from memory import ReplayBuffer
from models import Actor, Critic
from evaluation import evaluate_for_validation

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# --- TD3 í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
BUFFER_SIZE = int(1e6)
BATCH_SIZE = 128
GAMMA = 0.99
TAU = 1e-3
LR_ACTOR = 1e-4  # í•™ìŠµë¥  ì¡°ì •
LR_CRITIC = 1e-3 # í•™ìŠµë¥  ì¡°ì •

# --- TD3 ê´€ë ¨ ì‹ ê·œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
POLICY_NOISE = 0.2      # íƒ€ê²Ÿ ì •ì±… ìŠ¤ë¬´ë”© ë…¸ì´ì¦ˆ
NOISE_CLIP = 0.5        # ìŠ¤ë¬´ë”© ë…¸ì´ì¦ˆ í´ë¦¬í•‘ ë²”ìœ„
POLICY_FREQ = 2         # ì •ì±…(Actor) ì—…ë°ì´íŠ¸ ì£¼ê¸° (2ë²ˆì˜ Critic ì—…ë°ì´íŠ¸ë§ˆë‹¤ 1ë²ˆ ì—…ë°ì´íŠ¸)

class Agent():
    """TD3 ì—ì´ì „íŠ¸: DDPGì˜ ì•ˆì •ì„±ì„ ê°œì„ í•œ ë²„ì „."""

    def __init__(self, state_size, action_size, random_seed):
        self.state_size = state_size
        self.action_size = action_size
        self.seed = np.random.seed(random_seed)
        torch.manual_seed(random_seed)

        # Actor Network (ë¡œì»¬ ë° íƒ€ê²Ÿ)
        self.actor_local = Actor(state_size, action_size).to(device)
        self.actor_target = Actor(state_size, action_size).to(device)
        self.actor_target.load_state_dict(self.actor_local.state_dict())
        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)

        # Critic Networks (ìŒë‘¥ì´ Critic: 1, 2)
        self.critic_1_local = Critic(state_size, action_size).to(device)
        self.critic_1_target = Critic(state_size, action_size).to(device)
        self.critic_1_target.load_state_dict(self.critic_1_local.state_dict())
        self.critic_1_optimizer = optim.Adam(self.critic_1_local.parameters(), lr=LR_CRITIC)

        self.critic_2_local = Critic(state_size, action_size).to(device)
        self.critic_2_target = Critic(state_size, action_size).to(device)
        self.critic_2_target.load_state_dict(self.critic_2_local.state_dict())
        self.critic_2_optimizer = optim.Adam(self.critic_2_local.parameters(), lr=LR_CRITIC)
        
        # ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, device, random_seed)
        self.t_step = 0

    def step(self, state, action, reward, next_state, done):
        self.memory.add(state, action, reward, next_state, done)
        if len(self.memory) > BATCH_SIZE:
            self.t_step += 1
            self.learn(self.memory.sample(), GAMMA)

    def act(self, state, exploration_std=0.1):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        self.actor_local.eval()
        with torch.no_grad():
            action = self.actor_local(state).cpu().data.numpy().flatten()
        self.actor_local.train()
        
        # í–‰ë™ì— íƒìƒ‰ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = np.random.normal(0, exploration_std, size=self.action_size)
        action = (action + noise).clip(0, 1)
        action /= (action.sum() + 1e-8)
        return action

    def learn(self, experiences, gamma):
        states, actions, rewards, next_states, dones = experiences

        # ---------------- CRITIC ì—…ë°ì´íŠ¸ ---------------- #
        # 1. íƒ€ê²Ÿ ì •ì±… ìŠ¤ë¬´ë”©: ë‹¤ìŒ í–‰ë™ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = torch.randn_like(actions).data.normal_(0, POLICY_NOISE).to(device).clamp(-NOISE_CLIP, NOISE_CLIP)
        actions_next = (self.actor_target(next_states) + noise).clamp(0, 1)
        
        # 2. ìŒë‘¥ì´ Critic: ë‹¤ìŒ ìƒíƒœì˜ Q-value ê³„ì‚° í›„ ë” ì‘ì€ ê°’ ì„ íƒ
        Q1_targets_next = self.critic_1_target(next_states, actions_next)
        Q2_targets_next = self.critic_2_target(next_states, actions_next)
        Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)
        
        # í˜„ì¬ ìƒíƒœì— ëŒ€í•œ ìµœì¢… Q-target ê³„ì‚°
        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))

        # Critic 1 ì—…ë°ì´íŠ¸
        Q1_expected = self.critic_1_local(states, actions)
        critic_1_loss = F.mse_loss(Q1_expected, Q_targets.detach())
        self.critic_1_optimizer.zero_grad()
        critic_1_loss.backward()
        self.critic_1_optimizer.step()

        # Critic 2 ì—…ë°ì´íŠ¸
        Q2_expected = self.critic_2_local(states, actions)
        critic_2_loss = F.mse_loss(Q2_expected, Q_targets.detach())
        self.critic_2_optimizer.zero_grad()
        critic_2_loss.backward()
        self.critic_2_optimizer.step()

        # ---------------- ACTOR ì§€ì—° ì—…ë°ì´íŠ¸ ---------------- #
        if self.t_step % POLICY_FREQ == 0:
            # Actor ì†ì‹¤ ê³„ì‚°
            actions_pred = self.actor_local(states)
            actor_loss = -self.critic_1_local(states, actions_pred).mean()
            
            # Actor ì—…ë°ì´íŠ¸
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            # Target ë„¤íŠ¸ì›Œí¬ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸
            self.soft_update(self.critic_1_local, self.critic_1_target, TAU)
            self.soft_update(self.critic_2_local, self.critic_2_target, TAU)
            self.soft_update(self.actor_local, self.actor_target, TAU)

    def soft_update(self, local_model, target_model, tau):
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)
            
    # train í•¨ìˆ˜ëŠ” ì¡°ê¸° ì¢…ë£Œ ë¡œì§ì„ í¬í•¨í•˜ì—¬ ë” ê°œì„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì´ì „ ë‹µë³€ì˜ train í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    def train(self, train_env, validation_env, n_episodes, model_path, initial_exploration_std=0.2, min_exploration_std=0.05):
        scores_deque = deque(maxlen=10)
        best_validation_score = -np.inf
        
        # íƒìƒ‰ ë…¸ì´ì¦ˆ ê°ì†Œ(Decay)ë¥¼ ìœ„í•œ ì„¤ì •
        exploration_decay_rate = (initial_exploration_std - min_exploration_std) / n_episodes
        
        progress_bar = tqdm(range(1, n_episodes + 1), desc="Training Progress (TD3)")
        for i_episode in progress_bar:
            state = train_env.reset()
            score = 0
            done = False
            
            # í˜„ì¬ ì—í”¼ì†Œë“œì˜ íƒìƒ‰ ë…¸ì´ì¦ˆ í¬ê¸° ê³„ì‚°
            current_exploration_std = initial_exploration_std - (i_episode * exploration_decay_rate)
            
            while not done:
                action = self.act(state, exploration_std=current_exploration_std)
                next_state, reward, done, _ = train_env.step(action)
                self.step(state, action, reward, next_state, done)
                state = next_state
                score += reward
                if done: break
            
            scores_deque.append(score)
            avg_score = np.mean(scores_deque)
            progress_bar.set_postfix({"Avg Train Score": f"{avg_score:.2f}", "Exploration": f"{current_exploration_std:.3f}"})

            if i_episode % 5 == 0:
                validation_score = evaluate_for_validation(validation_env, self)
                tqdm.write(f"\nEpisode {i_episode}\tAvg Train Score: {avg_score:.2f}\tValidation Sharpe Ratio: {validation_score:.4f}")
                if validation_score > best_validation_score:
                    best_validation_score = validation_score
                    tqdm.write(f"ğŸ‰ New best model found! Saving model to {model_path}")
                    torch.save(self.actor_local.state_dict(), os.path.join(model_path, 'best_actor.pth'))
                    torch.save(self.critic_1_local.state_dict(), os.path.join(model_path, 'best_critic.pth')) # Criticì€ í•˜ë‚˜ë§Œ ì €ì¥í•´ë„ ë¬´ë°©